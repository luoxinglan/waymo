{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "cYLZzIbjk62B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Waymo Open Dataset Motion Tutorial\n",
    "\n",
    "- Website: https://waymo.com/open\n",
    "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
    "\n",
    "This tutorial demonstrates:\n",
    "- How to decode and interpret the data.\n",
    "- How to train a simple model with Tensorflow.\n",
    "\n",
    "Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
    "\n",
    "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
    "\n",
    "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ez4Nsk06Sqd"
   },
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T06:56:50.009825Z",
     "start_time": "2025-04-01T06:56:49.342236Z"
    },
    "id": "WH4E-2DySUbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\r\n",
      "Requirement already satisfied: waymo-open-dataset-tf-2-12-0==1.6.4 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (1.6.4)\r\n",
      "Requirement already satisfied: absl-py==1.4.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.0)\r\n",
      "Requirement already satisfied: dask==2023.3.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2023.3.1)\r\n",
      "Requirement already satisfied: einsum==0.3.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.3.0)\r\n",
      "Requirement already satisfied: google-auth==2.16.2 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.16.2)\r\n",
      "Requirement already satisfied: immutabledict==2.2.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.2.0)\r\n",
      "Requirement already satisfied: matplotlib==3.6.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (3.6.1)\r\n",
      "Requirement already satisfied: numpy==1.23 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.23.0)\r\n",
      "Requirement already satisfied: openexr==1.3.9 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.3.9)\r\n",
      "Requirement already satisfied: pandas==1.5.3 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.5.3)\r\n",
      "Requirement already satisfied: pillow==9.2.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (9.2.0)\r\n",
      "Requirement already satisfied: plotly==5.13.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (5.13.1)\r\n",
      "Requirement already satisfied: pyarrow==10.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (10.0.0)\r\n",
      "Requirement already satisfied: scikit-image==0.20.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.20.0)\r\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.2.2)\r\n",
      "Requirement already satisfied: setuptools==67.6.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (67.6.0)\r\n",
      "Requirement already satisfied: tensorflow==2.12 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\r\n",
      "Requirement already satisfied: tensorflow_graphics==2021.12.3 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (2021.12.3)\r\n",
      "Requirement already satisfied: tensorflow_probability==0.19.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (0.19.0)\r\n",
      "Requirement already satisfied: visu3d==1.5.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.5.1)\r\n",
      "Requirement already satisfied: dacite==1.8.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from waymo-open-dataset-tf-2-12-0==1.6.4) (1.8.1)\r\n",
      "Requirement already satisfied: click>=7.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (8.1.8)\r\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.1.1)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2025.3.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (24.2)\r\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (6.0.2)\r\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.2)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (1.17.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (4.9)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.7)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from matplotlib==3.6.1->waymo-open-dataset-tf-2-12-0==1.6.4) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from pandas==1.5.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2025.2)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from plotly==5.13.1->waymo-open-dataset-tf-2-12-0==1.6.4) (9.0.0)\r\n",
      "Requirement already satisfied: scipy<1.9.2,>=1.8 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (1.9.1)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (3.1)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (2.35.1)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (2023.7.10)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.1)\r\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-image==0.20.0->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-learn==1.2.2->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from scikit-learn==1.2.2->waymo-open-dataset-tf-2-12-0==1.6.4) (3.5.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (25.2.10)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.70.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.11.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.13)\r\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.4.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.20.3)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.12.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (4.13.0)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.34.0)\r\n",
      "Requirement already satisfied: tensorflow-addons>=0.10.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.21.0)\r\n",
      "Requirement already satisfied: tensorflow-datasets>=2.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.9.2)\r\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (7.0.0)\r\n",
      "Requirement already satisfied: tqdm>=4.45.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.67.1)\r\n",
      "Requirement already satisfied: trimesh>=2.37.22 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (4.6.5)\r\n",
      "Requirement already satisfied: decorator in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_probability==0.19.0->waymo-open-dataset-tf-2-12-0==1.6.4) (5.2.1)\r\n",
      "Requirement already satisfied: dm-tree in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow_probability==0.19.0->waymo-open-dataset-tf-2-12-0==1.6.4) (0.1.8)\r\n",
      "Requirement already satisfied: dataclass_array in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.4.1)\r\n",
      "Requirement already satisfied: einops in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (0.8.1)\r\n",
      "Requirement already satisfied: etils[edc,enp,epath,epy,etree] in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.3.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.44.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.2.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (8.5.0)\r\n",
      "Requirement already satisfied: locket in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from partd>=1.2.0->dask==2023.3.1->dask[dataframe]==2023.3.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth==2.16.2->waymo-open-dataset-tf-2-12-0==1.6.4) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.7)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.32.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.0.6)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-addons>=0.10.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2.13.3)\r\n",
      "Requirement already satisfied: array-record in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.4.0)\r\n",
      "Requirement already satisfied: promise in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (2.3)\r\n",
      "Requirement already satisfied: tensorflow-metadata in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (1.14.0)\r\n",
      "Requirement already satisfied: toml in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (0.10.2)\r\n",
      "Requirement already satisfied: importlib-resources in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (6.4.5)\r\n",
      "Requirement already satisfied: lark in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from dataclass_array->visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (1.2.2)\r\n",
      "Requirement already satisfied: zipp in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from etils[edc,enp,epath,epy,etree]->visu3d==1.5.1->waymo-open-dataset-tf-2-12-0==1.6.4) (3.20.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2025.1.31)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (2.1.5)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets>=2.0.0->tensorflow_graphics==2021.12.3->waymo-open-dataset-tf-2-12-0==1.6.4) (1.69.2)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/heihuhu/anaconda3/envs/waymo/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->waymo-open-dataset-tf-2-12-0==1.6.4) (3.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install waymo-open-dataset-tf-2-12-0==1.6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjT3Rdd4lSqC"
   },
   "source": [
    "# Imports and global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:39:59.679491Z",
     "start_time": "2025-04-02T08:39:59.676949Z"
    },
    "id": "xdEcN6WilcBn"
   },
   "outputs": [],
   "source": [
    "# Data location. Please edit.\n",
    "\n",
    "# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset\n",
    "# webpage.\n",
    "\n",
    "# Replace this path with your own tfrecords.\n",
    "FILENAME = '/home/heihuhu/PycharmProjects/waymo/data/uncompressed_tf_example_training_training_tfexample.tfrecord-00000-of-01000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:00.252771Z",
     "start_time": "2025-04-02T08:40:00.242164Z"
    },
    "editable": true,
    "id": "M5gzSlBTlTiS",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
    "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
    "from waymo_open_dataset.protos import motion_metrics_pb2\n",
    "\n",
    "# If you use a custom conversion from Scenario to tf.Example, set the correct\n",
    "# number of map samples here.\n",
    "num_map_samples = 30000\n",
    "\n",
    "# Example field definition\n",
    "roadgraph_features = {\n",
    "    'roadgraph_samples/dir': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 3], tf.float32, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/id': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/type': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/valid': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 1], tf.int64, default_value=None\n",
    "    ),\n",
    "    'roadgraph_samples/xyz': tf.io.FixedLenFeature(\n",
    "        [num_map_samples, 3], tf.float32, default_value=None\n",
    "    ),\n",
    "}\n",
    "# Features of other agents.\n",
    "state_features = {\n",
    "    'state/id':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/type':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/is_sdc':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/tracks_to_predict':  #一个标志向量，用于指示要预测哪些对象对应于 state/XX 张量的第一个维度。最多可选择 8 个对象。对于训练集和验证集，这些指示提交 proto prediction 中必须存在哪个对象。如果在提交 proto 中没有准确预测这些对象，则提交将失败。此字段也存在于训练集中，作为要训练的对象的参考，但您可以自由选择要训练的其他对象。\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/objects_of_interest':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "\n",
    "    'state/current/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/height':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/length':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/timestamp_micros':  # 每个时间步长的时间戳（以微秒为单位）。\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/valid':  #功能 state/current/XX 的所有元素的有效标志。如果设置为 1，则元素将填充有效数据，否则将填充 -1。{0, 1}.指示 state/current/XX 张量的每个元素的数据是否填充了有效数据（而不是用 -1 填充）。\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/vel_yaw':  #每个对象的速度向量在每个时间步长的偏转角。角度（以弧度为单位）。这是在 X-Y 平面中沿 X 方向旋转单位向量以与对象边界框的速度向量对齐的角度。\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_x':  #每个时间步长处对象速度的 x 分量。\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/width':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/z':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/speed':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "\n",
    "    'state/future/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/height':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/length':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/valid':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/width':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/z':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/speed':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "\n",
    "    'state/past/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/height':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/length':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/valid':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/width':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/z':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/speed':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "traffic_light_features = {\n",
    "    'traffic_light_state/current/state':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/valid':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/id':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([1], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/x':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/y':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/z':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "\n",
    "    'traffic_light_state/future/state':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/future/valid':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/future/id':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/future/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([80], tf.int64, default_value=None),\n",
    "    'traffic_light_state/future/x':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/future/y':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/future/z':\n",
    "        tf.io.FixedLenFeature([80, 16], tf.float32, default_value=None),\n",
    "\n",
    "    'traffic_light_state/past/state':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/valid':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/id':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([10], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/x':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/y':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/z':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "features_description = {}\n",
    "features_description.update(roadgraph_features)\n",
    "features_description.update(state_features)\n",
    "features_description.update(traffic_light_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trAv9YGrvYnc"
   },
   "source": [
    "# Visualize TF Example sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWnysu4X7Wkt"
   },
   "source": [
    "## Create Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:01.741569Z",
     "start_time": "2025-04-02T08:40:01.724388Z"
    },
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "data = next(dataset.as_numpy_iterator())\n",
    "parsed = tf.io.parse_single_example(data, features_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据结构\n",
    "\n",
    "展示parsed是什么东西。`state/objects_of_interest`表示与主车有交互的两辆车，而不是一辆交互车一辆主车。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:03.292846Z",
     "start_time": "2025-04-02T08:40:03.289417Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"type of parsed: {type(parsed)}\")\n",
    "print(f\"length of parsed: {len(parsed)}\")\n",
    "\n",
    "print(f\"traffic_light_state/past/z: {parsed['traffic_light_state/past/z']}\")\n",
    "print(f\"shape of traffic_light_state/past/z: {parsed['traffic_light_state/past/z'].shape}\")\n",
    "\n",
    "print(f\"state/objects_of_interest: {parsed['state/objects_of_interest']}\")\n",
    "print(f\"shape of state/objects_of_interest: {parsed['state/objects_of_interest'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看主车id、跟主车有交互的车辆。**\n",
    "\n",
    "`state/id`: An integer ID for each object.\n",
    "\n",
    "`state/is_sdc`: A mask to indicate if the object is the autonomous vehicle.\n",
    "\n",
    "查找主车的ID，然后注意到所有有交互的车辆都是与主车交互的——数据集已经给出每帧与主车交互的actor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:04.147745Z",
     "start_time": "2025-04-02T08:40:04.143362Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"state/id: {parsed['state/id']}\")\n",
    "print(f\"shape of state/id: {parsed['state/id'].shape}\")\n",
    "\n",
    "print(f\"state/is_sdc: {parsed['state/is_sdc']}\")\n",
    "print(f\"shape of state/is_sdc: {parsed['state/is_sdc'].shape}\")\n",
    "\n",
    "sdc_index = np.where(parsed['state/is_sdc'].numpy() == 1)[0]\n",
    "print(f\"sdc_index: {sdc_index}\")\n",
    "if len(sdc_index) > 0:\n",
    "    sdc_index = sdc_index[0]  # 取第一个主车（通常只有一个）\n",
    "else:\n",
    "    sdc_index = None  # 无主车的情况\n",
    "\n",
    "print(f\"主车ID为：{parsed['state/id'][sdc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**展示type，查看不同类型的actor**\n",
    "\n",
    "`state/type`: An integer type for each object (Vehicle, Pedestrian, or Cyclist)\n",
    "- Unset=0, Vehicle=1, Pedestrian=2, Cyclist=3, Other=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:05.033958Z",
     "start_time": "2025-04-02T08:40:05.031053Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"state/type: {parsed['state/type']}\")\n",
    "print(f\"shape of state/type: {parsed['state/type'].shape}\")\n",
    "\n",
    "print(f\"state/tracks_to_predict: {parsed['state/tracks_to_predict']}\")\n",
    "print(f\"shape of state/tracks_to_predict: {parsed['state/tracks_to_predict'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别获取不同种类actor的下标\n",
    "\n",
    "配色方案\n",
    "- Unset=0(gray,spot),\n",
    "- Vehicle=1(blue,spot),\n",
    "    - interest(靛蓝色)\n",
    "    - sdc(red spot)\n",
    "- Pedestrian=2(yellow,spot),\n",
    "- Cyclist=3(green,spot),\n",
    "- Other=4(brown,spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:06.271912Z",
     "start_time": "2025-04-02T08:40:06.267762Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_actor_indices(decoded_example):\n",
    "    \"\"\"\n",
    "    根据 state/type 数组，返回不同类型的 actor 的下标。\n",
    "    :param decoded_example: 处理好的数据\n",
    "    :return:\n",
    "        {\n",
    "            'Vehicle': [indices],\n",
    "            'Pedestrian': [indices],\n",
    "            'Cyclist': [indices],\n",
    "            'Other': [indices]\n",
    "        }\n",
    "    \"\"\"\n",
    "    state_type_array = decoded_example['state/type']\n",
    "    # 确保输入是 NumPy 数组\n",
    "    if not isinstance(state_type_array, np.ndarray):\n",
    "        state_type_array = np.array(state_type_array)\n",
    "\n",
    "    # 新增：获取主车索引\n",
    "    sdc_index = np.where(decoded_example['state/is_sdc'].numpy() == 1)[0]\n",
    "    print(f\"sdc_index{sdc_index}\")\n",
    "    if len(sdc_index) > 0:\n",
    "        sdc_index = sdc_index[0]  # 取第一个主车（通常只有一个）\n",
    "    else:\n",
    "        sdc_index = None  # 无主车的情况\n",
    "\n",
    "    # 新增：获取交互车辆索引\n",
    "    objects_of_interest = np.where(decoded_example['state/objects_of_interest'].numpy() == 1)[0]\n",
    "    print(f\"objects_of_interest: {objects_of_interest}\")\n",
    "\n",
    "    # 新增：获取to predict车辆索引\n",
    "    tracks_to_predict = np.where(decoded_example['state/tracks_to_predict'].numpy() == 1)[0]\n",
    "    print(f\"state/tracks_to_predict: {tracks_to_predict}\")\n",
    "\n",
    "    # 使用 NumPy 的向量化操作快速获取下标\n",
    "    indices = {\n",
    "        'Vehicle': np.where(state_type_array == 1)[0].tolist(),\n",
    "        'Pedestrian': np.where(state_type_array == 2)[0].tolist(),\n",
    "        'Cyclist': np.where(state_type_array == 3)[0].tolist(),\n",
    "        'Other': np.where(state_type_array == 4)[0].tolist(),\n",
    "        'sdc_index': sdc_index,\n",
    "        'objects_of_interest': objects_of_interest,\n",
    "        'tracks_to_predict': tracks_to_predict\n",
    "    }\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "index = get_actor_indices(parsed)\n",
    "print(index)\n",
    "print(index['objects_of_interest'])\n",
    "print(index['tracks_to_predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Zdc8CBg27dtn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Generate visualization images.\n",
    "\n",
    "- 将车辆与其他类型的actor区分开绘制。\n",
    "- 将交互车辆和主车区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:09.524259Z",
     "start_time": "2025-04-02T08:40:09.513563Z"
    },
    "id": "utTE9Mtgx3Fq"
   },
   "outputs": [],
   "source": [
    "def create_figure_and_axes(size_pixels):\n",
    "    \"\"\"Initializes a unique figure and axes for plotting.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
    "\n",
    "    # Sets output image to pixel resolution.\n",
    "    dpi = 100\n",
    "    size_inches = size_pixels / dpi\n",
    "    fig.set_size_inches([size_inches, size_inches])\n",
    "    fig.set_dpi(dpi)\n",
    "    fig.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    ax.xaxis.label.set_color('black')\n",
    "    ax.tick_params(axis='x', colors='black')\n",
    "    ax.yaxis.label.set_color('black')\n",
    "    ax.tick_params(axis='y', colors='black')\n",
    "    fig.set_tight_layout(True)\n",
    "    ax.grid(False)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def fig_canvas_image(fig):\n",
    "    \"\"\"Returns a [H, W, 3] uint8 np.array image from fig.canvas.tostring_rgb().\"\"\"\n",
    "    # Just enough margin in the figure to display xticks and yticks.\n",
    "    fig.subplots_adjust(\n",
    "        left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
    "    fig.canvas.draw()\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "\n",
    "def get_colormap(num_agents):\n",
    "    \"\"\"Compute a color map array of shape [num_agents, 4].\"\"\"\n",
    "    colors = cm.get_cmap('jet', num_agents)\n",
    "    colors = colors(range(num_agents))\n",
    "    np.random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def get_viewport(all_states, all_states_mask):\n",
    "    \"\"\"Gets the region containing the data.\n",
    "\n",
    "    Args:\n",
    "      all_states: states of agents as an array of shape [num_agents, num_steps,\n",
    "        2].\n",
    "      all_states_mask: binary mask of shape [num_agents, num_steps] for\n",
    "        `all_states`.\n",
    "\n",
    "    Returns:\n",
    "      center_y: float. y coordinate for center of data.\n",
    "      center_x: float. x coordinate for center of data.\n",
    "      width: float. Width of data.\n",
    "    \"\"\"\n",
    "    valid_states = all_states[all_states_mask]\n",
    "    all_y = valid_states[..., 1]\n",
    "    all_x = valid_states[..., 0]\n",
    "\n",
    "    center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
    "    center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
    "\n",
    "    range_y = np.ptp(all_y)\n",
    "    range_x = np.ptp(all_x)\n",
    "\n",
    "    width = max(range_y, range_x)\n",
    "\n",
    "    return center_y, center_x, width\n",
    "\n",
    "\n",
    "def visualize_one_step(states,\n",
    "                       mask,\n",
    "                       roadgraph,\n",
    "                       title,\n",
    "                       center_y,\n",
    "                       center_x,\n",
    "                       width,\n",
    "                       color_map,\n",
    "                       size_pixels=1000,\n",
    "                       actor_indices=None,\n",
    "                       decoded_example=None):\n",
    "    \"\"\"Generate visualization for a single step.\"\"\"\n",
    "\n",
    "    # Create figure and axes.\n",
    "    fig, ax = create_figure_and_axes(size_pixels=size_pixels)\n",
    "\n",
    "    # Plot roadgraph.\n",
    "    rg_pts = roadgraph[:, :2].T\n",
    "    ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
    "\n",
    "    # Define markers and colors based on actor type\n",
    "    markers = ['o', 'o', 'o', 'o']  # Default marker is circle 'o'\n",
    "    # print(f\"markers: {markers[1]}\")\n",
    "    colors = [(0.5, 0.5, 0.5), (0, 0, 1), (1, 1, 0), (0, 1, 0), (0.6, 0.4, 0.2)]  # Gray, Blue, Yellow, Green, Brown\n",
    "\n",
    "    masked_x = states[:, 0][mask]\n",
    "    masked_y = states[:, 1][mask]\n",
    "    types = decoded_example['state/type'][mask].numpy().astype(int)\n",
    "    # print(f\"types: {types}, type of types: {type(types)}\")\n",
    "\n",
    "    # Plot agent current position.\n",
    "    # 绘制普通代理\n",
    "    for idx, (x, y, t) in enumerate(zip(masked_x, masked_y, types)):\n",
    "        # print(f\"idx: {idx}, x: {x}, y: {y}, t: {t}\")\n",
    "        # print(f\"markers[t - 1]:{markers[t - 1]}\")\n",
    "        ax.scatter(x, y,\n",
    "                   marker=markers[t - 1],\n",
    "                   s=50,  # 普通代理大小\n",
    "                   linewidths=1.5,\n",
    "                   color=colors[t],\n",
    "                   alpha=0.7)\n",
    "    # 如果存在主车索引，单独绘制\n",
    "    # If there exists main car index, plot it separately\n",
    "    if actor_indices and 'sdc_index' in actor_indices and actor_indices['sdc_index'] is not None and mask[\n",
    "        actor_indices['sdc_index']]:\n",
    "        sdc_x = states[actor_indices['sdc_index'], 0]\n",
    "        sdc_y = states[actor_indices['sdc_index'], 1]\n",
    "        ax.scatter([sdc_x], [sdc_y],\n",
    "                   marker='*',  # Star marker\n",
    "                   s=500,  # Large size\n",
    "                   linewidths=1,\n",
    "                   color='red',  # Red\n",
    "                   edgecolors='black',  # Black border\n",
    "                   zorder=3)  # Ensure it's on top\n",
    "\n",
    "    # 如果存在有需要预测轨迹的车辆，单独绘制\n",
    "    # If there exist interacting vehicles, plot them separately\n",
    "    if actor_indices and 'tracks_to_predict' in actor_indices:\n",
    "        for predict in actor_indices['tracks_to_predict']:\n",
    "            if predict is not None and mask[predict]:\n",
    "                predict_x = states[predict, 0]\n",
    "                predict_y = states[predict, 1]\n",
    "                ax.scatter([predict_x], [predict_y],\n",
    "                           marker='o',  # Star marker\n",
    "                           s=500,  # Large size\n",
    "                           linewidths=1,\n",
    "                           color='#d215e1',  # Indigo blue\n",
    "                           # color=(0, 0, 0.5),  # Indigo blue\n",
    "                           edgecolors='black',  # Black border\n",
    "                           zorder=3)  # Ensure it's on top\n",
    "    \n",
    "    # 如果存在有交互的车辆，单独绘制\n",
    "    # If there exist interacting vehicles, plot them separately\n",
    "    if actor_indices and 'objects_of_interest' in actor_indices:\n",
    "        for interest in actor_indices['objects_of_interest']:\n",
    "            if interest is not None and mask[interest]:\n",
    "                interest_x = states[interest, 0]\n",
    "                interest_y = states[interest, 1]\n",
    "                ax.scatter([interest_x], [interest_y],\n",
    "                           marker='*',  # Star marker\n",
    "                           s=500,  # Large size\n",
    "                           linewidths=1,\n",
    "                           color=(0, 0, 0.5),  # Indigo blue\n",
    "                           edgecolors='black',  # Black border\n",
    "                           zorder=3)  # Ensure it's on top\n",
    "\n",
    "    # Title.\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
    "    size = max(10, width * 1.0)\n",
    "    ax.axis([\n",
    "        -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
    "        size / 2 + center_y\n",
    "    ])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    image = fig_canvas_image(fig)\n",
    "    plt.close(fig)\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_all_agents_smooth(\n",
    "        decoded_example,\n",
    "        actor_indices=None,\n",
    "        size_pixels=1000,\n",
    "):\n",
    "    \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
    "\n",
    "    Args:\n",
    "      decoded_example: Dictionary containing agent info about all modeled agents.\n",
    "      size_pixels: The size in pixels of the output image.\n",
    "\n",
    "    Returns:\n",
    "      T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
    "    \"\"\"\n",
    "    # [num_agents, num_past_steps, 2] float32.\n",
    "    past_states = tf.stack(\n",
    "        [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
    "        -1).numpy()\n",
    "    past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
    "\n",
    "    # [num_agents, 1, 2] float32.\n",
    "    current_states = tf.stack(\n",
    "        [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
    "        -1).numpy()\n",
    "    current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
    "\n",
    "    # [num_agents, num_future_steps, 2] float32.\n",
    "    future_states = tf.stack(\n",
    "        [decoded_example['state/future/x'], decoded_example['state/future/y']],\n",
    "        -1).numpy()\n",
    "    future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
    "\n",
    "    # [num_points, 3] float32.\n",
    "    roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
    "\n",
    "    num_agents, num_past_steps, _ = past_states.shape\n",
    "    num_future_steps = future_states.shape[1]\n",
    "\n",
    "    color_map = get_colormap(num_agents)\n",
    "\n",
    "    # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
    "    all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
    "\n",
    "    # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
    "    all_states_mask = np.concatenate(\n",
    "        [past_states_mask, current_states_mask, future_states_mask], 1)\n",
    "\n",
    "    center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
    "\n",
    "    images = []\n",
    "\n",
    "    # Generate images from past time steps.\n",
    "\n",
    "    for i, (s, m) in enumerate(\n",
    "            zip(np.split(past_states, num_past_steps, 1), np.split(past_states_mask, num_past_steps, 1))):\n",
    "        # print(f\"i: {i}, s: {s}, m: {m}\")\n",
    "        im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                                'past: %d' % (num_past_steps - i), center_y,\n",
    "                                center_x, width, color_map, size_pixels, actor_indices, decoded_example)\n",
    "        images.append(im)\n",
    "\n",
    "    # Generate one image for the current time step.\n",
    "    s = current_states\n",
    "    m = current_states_mask\n",
    "\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
    "                            center_x, width, color_map, size_pixels, actor_indices, decoded_example)\n",
    "    images.append(im)\n",
    "\n",
    "    # Generate images from future time steps.\n",
    "    for i, (s, m) in enumerate(\n",
    "            zip(\n",
    "                np.split(future_states, num_future_steps, 1),\n",
    "                np.split(future_states_mask, num_future_steps, 1))):\n",
    "        im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                                'future: %d' % (i + 1), center_y, center_x, width,\n",
    "                                color_map, size_pixels, actor_indices, decoded_example)\n",
    "        images.append(im)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:40:30.098167Z",
     "start_time": "2025-04-02T08:40:10.410762Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取actor索引\n",
    "actor_indices = get_actor_indices(parsed)\n",
    "# 可视化所有代理的平滑轨迹\n",
    "images = visualize_all_agents_smooth(parsed, actor_indices=actor_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrIZjUHG7hM3"
   },
   "source": [
    "## Display animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:43:58.737164Z",
     "start_time": "2025-04-02T08:40:37.834326Z"
    },
    "id": "tt2IeGiG0eny"
   },
   "outputs": [],
   "source": [
    "def create_animation(images):\n",
    "    \"\"\" Creates a Matplotlib animation of the given images.\n",
    "\n",
    "    Args:\n",
    "      images: A list of numpy arrays representing the images.\n",
    "\n",
    "    Returns:\n",
    "      A matplotlib.animation.Animation.\n",
    "\n",
    "    Usage:\n",
    "      anim = create_animation(images)\n",
    "      anim.save('/tmp/animation.avi')\n",
    "      HTML(anim.to_html5_video())\n",
    "    \"\"\"\n",
    "\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots()\n",
    "    dpi = 100\n",
    "    size_inches = 1000 / dpi\n",
    "    fig.set_size_inches([size_inches, size_inches])\n",
    "    plt.ion()\n",
    "\n",
    "    def animate_func(i):\n",
    "        ax.imshow(images[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.grid('off')\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate_func, frames=len(images) // 2, interval=100)\n",
    "    plt.close(fig)\n",
    "    return anim\n",
    "\n",
    "\n",
    "anim = create_animation(images[::1])\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "anim.save('./video/animation.mp4',\n",
    "          writer=animation.FFMpegWriter(fps=10, bitrate=5000),\n",
    "          dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "wdOQTZAiuKdQ",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Simple MLP model with TF\n",
    "\n",
    "Note that this is a very simple example model to demonstrate inputs parsing and metrics computation. Not at all competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T08:44:00.571807Z",
     "start_time": "2025-04-02T08:43:58.791311Z"
    },
    "id": "b_5G9lx9uK9B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 1136246.2500\n",
      "Seen so far: 64 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 16:43:58.875252: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 10: 1086275.2500\n",
      "Seen so far: 704 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 16:43:59.230470: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:163] Computing motion metrics for 352 trajectories.\n",
      "2025-04-02 16:43:59.230485: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:166] Parsing prediction [352,128,1,1,16,2][352]\n",
      "2025-04-02 16:43:59.230487: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:170] Parsing ground truth [352,128,91,7][352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_ade/TYPE_VEHICLE_5: 7123.4375\n",
      "min_ade/TYPE_VEHICLE_9: 6725.51220703125\n",
      "min_ade/TYPE_VEHICLE_15: 6519.36474609375\n",
      "min_ade/TYPE_PEDESTRIAN_5: 6543.85009765625\n",
      "min_ade/TYPE_PEDESTRIAN_9: 6164.64111328125\n",
      "min_ade/TYPE_PEDESTRIAN_15: 6084.38818359375\n",
      "min_ade/TYPE_CYCLIST_5: 6483.07763671875\n",
      "min_ade/TYPE_CYCLIST_9: 6077.8681640625\n",
      "min_ade/TYPE_CYCLIST_15: 6036.91650390625\n",
      "min_fde/TYPE_VEHICLE_5: 8301.400390625\n",
      "min_fde/TYPE_VEHICLE_9: 4216.564453125\n",
      "min_fde/TYPE_VEHICLE_15: 8046.22509765625\n",
      "min_fde/TYPE_PEDESTRIAN_5: 7154.17041015625\n",
      "min_fde/TYPE_PEDESTRIAN_9: 3798.66064453125\n",
      "min_fde/TYPE_PEDESTRIAN_15: 7715.14501953125\n",
      "min_fde/TYPE_CYCLIST_5: 6873.97705078125\n",
      "min_fde/TYPE_CYCLIST_9: 3732.183837890625\n",
      "min_fde/TYPE_CYCLIST_15: 7395.7392578125\n",
      "miss_rate/TYPE_VEHICLE_5: 1.0\n",
      "miss_rate/TYPE_VEHICLE_9: 1.0\n",
      "miss_rate/TYPE_VEHICLE_15: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_5: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_9: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_15: 1.0\n",
      "miss_rate/TYPE_CYCLIST_5: 1.0\n",
      "miss_rate/TYPE_CYCLIST_9: 1.0\n",
      "miss_rate/TYPE_CYCLIST_15: 1.0\n",
      "overlap_rate/TYPE_VEHICLE_5: 0.0\n",
      "overlap_rate/TYPE_VEHICLE_9: 0.0\n",
      "overlap_rate/TYPE_VEHICLE_15: 0.0\n",
      "overlap_rate/TYPE_PEDESTRIAN_5: 0.0\n",
      "overlap_rate/TYPE_PEDESTRIAN_9: 0.0\n",
      "overlap_rate/TYPE_PEDESTRIAN_15: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_5: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_9: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_15: 0.0\n",
      "map/TYPE_VEHICLE_5: 0.0\n",
      "map/TYPE_VEHICLE_9: 0.0\n",
      "map/TYPE_VEHICLE_15: 0.0\n",
      "map/TYPE_PEDESTRIAN_5: 0.0\n",
      "map/TYPE_PEDESTRIAN_9: 0.0\n",
      "map/TYPE_PEDESTRIAN_15: 0.0\n",
      "map/TYPE_CYCLIST_5: 0.0\n",
      "map/TYPE_CYCLIST_9: 0.0\n",
      "map/TYPE_CYCLIST_15: 0.0\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 916013.5000\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 10: 866368.8125\n",
      "Seen so far: 704 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 16:43:59.800934: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:163] Computing motion metrics for 704 trajectories.\n",
      "2025-04-02 16:43:59.800958: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:166] Parsing prediction [704,128,1,1,16,2][704]\n",
      "2025-04-02 16:43:59.800960: I waymo_open_dataset/metrics/ops/motion_metrics_ops.cc:170] Parsing ground truth [704,128,91,7][704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_ade/TYPE_VEHICLE_5: 6701.23193359375\n",
      "min_ade/TYPE_VEHICLE_9: 6296.15234375\n",
      "min_ade/TYPE_VEHICLE_15: 6108.466796875\n",
      "min_ade/TYPE_PEDESTRIAN_5: 6186.53955078125\n",
      "min_ade/TYPE_PEDESTRIAN_9: 5806.89794921875\n",
      "min_ade/TYPE_PEDESTRIAN_15: 5729.5048828125\n",
      "min_ade/TYPE_CYCLIST_5: 6149.83837890625\n",
      "min_ade/TYPE_CYCLIST_9: 5748.40576171875\n",
      "min_ade/TYPE_CYCLIST_15: 5706.15625\n",
      "min_fde/TYPE_VEHICLE_5: 7843.1142578125\n",
      "min_fde/TYPE_VEHICLE_9: 3745.79052734375\n",
      "min_fde/TYPE_VEHICLE_15: 7606.24072265625\n",
      "min_fde/TYPE_PEDESTRIAN_5: 6784.01318359375\n",
      "min_fde/TYPE_PEDESTRIAN_9: 3420.789306640625\n",
      "min_fde/TYPE_PEDESTRIAN_15: 7333.03955078125\n",
      "min_fde/TYPE_CYCLIST_5: 6539.12353515625\n",
      "min_fde/TYPE_CYCLIST_9: 3383.37890625\n",
      "min_fde/TYPE_CYCLIST_15: 7046.1484375\n",
      "miss_rate/TYPE_VEHICLE_5: 1.0\n",
      "miss_rate/TYPE_VEHICLE_9: 1.0\n",
      "miss_rate/TYPE_VEHICLE_15: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_5: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_9: 1.0\n",
      "miss_rate/TYPE_PEDESTRIAN_15: 1.0\n",
      "miss_rate/TYPE_CYCLIST_5: 1.0\n",
      "miss_rate/TYPE_CYCLIST_9: 1.0\n",
      "miss_rate/TYPE_CYCLIST_15: 1.0\n",
      "overlap_rate/TYPE_VEHICLE_5: 0.0\n",
      "overlap_rate/TYPE_VEHICLE_9: 0.0\n",
      "overlap_rate/TYPE_VEHICLE_15: 0.00037650601007044315\n",
      "overlap_rate/TYPE_PEDESTRIAN_5: 0.0\n",
      "overlap_rate/TYPE_PEDESTRIAN_9: 0.0\n",
      "overlap_rate/TYPE_PEDESTRIAN_15: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_5: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_9: 0.0\n",
      "overlap_rate/TYPE_CYCLIST_15: 0.0\n",
      "map/TYPE_VEHICLE_5: 0.0\n",
      "map/TYPE_VEHICLE_9: 0.0\n",
      "map/TYPE_VEHICLE_15: 0.0\n",
      "map/TYPE_PEDESTRIAN_5: 0.0\n",
      "map/TYPE_PEDESTRIAN_9: 0.0\n",
      "map/TYPE_PEDESTRIAN_15: 0.0\n",
      "map/TYPE_CYCLIST_5: 0.0\n",
      "map/TYPE_CYCLIST_9: 0.0\n",
      "map/TYPE_CYCLIST_15: 0.0\n"
     ]
    }
   ],
   "source": [
    "def _parse(value):\n",
    "    decoded_example = tf.io.parse_single_example(value, features_description)\n",
    "\n",
    "    past_states = tf.stack([\n",
    "        decoded_example['state/past/x'], decoded_example['state/past/y'],\n",
    "        decoded_example['state/past/length'], decoded_example['state/past/width'],\n",
    "        decoded_example['state/past/bbox_yaw'],\n",
    "        decoded_example['state/past/velocity_x'],\n",
    "        decoded_example['state/past/velocity_y']\n",
    "    ], -1)\n",
    "\n",
    "    cur_states = tf.stack([\n",
    "        decoded_example['state/current/x'], decoded_example['state/current/y'],\n",
    "        decoded_example['state/current/length'],\n",
    "        decoded_example['state/current/width'],\n",
    "        decoded_example['state/current/bbox_yaw'],\n",
    "        decoded_example['state/current/velocity_x'],\n",
    "        decoded_example['state/current/velocity_y']\n",
    "    ], -1)\n",
    "\n",
    "    input_states = tf.concat([past_states, cur_states], 1)[..., :2]\n",
    "\n",
    "    future_states = tf.stack([\n",
    "        decoded_example['state/future/x'], decoded_example['state/future/y'],\n",
    "        decoded_example['state/future/length'],\n",
    "        decoded_example['state/future/width'],\n",
    "        decoded_example['state/future/bbox_yaw'],\n",
    "        decoded_example['state/future/velocity_x'],\n",
    "        decoded_example['state/future/velocity_y']\n",
    "    ], -1)\n",
    "\n",
    "    gt_future_states = tf.concat([past_states, cur_states, future_states], 1)\n",
    "\n",
    "    past_is_valid = decoded_example['state/past/valid'] > 0\n",
    "    current_is_valid = decoded_example['state/current/valid'] > 0\n",
    "    future_is_valid = decoded_example['state/future/valid'] > 0\n",
    "    gt_future_is_valid = tf.concat(\n",
    "        [past_is_valid, current_is_valid, future_is_valid], 1)\n",
    "\n",
    "    # If a sample was not seen at all in the past, we declare the sample as\n",
    "    # invalid.\n",
    "    sample_is_valid = tf.reduce_any(\n",
    "        tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
    "\n",
    "    inputs = {\n",
    "        'input_states': input_states,\n",
    "        'gt_future_states': gt_future_states,\n",
    "        'gt_future_is_valid': gt_future_is_valid,\n",
    "        'object_type': decoded_example['state/type'],\n",
    "        'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
    "        'sample_is_valid': sample_is_valid,\n",
    "    }\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def _default_metrics_config():\n",
    "    config = motion_metrics_pb2.MotionMetricsConfig()\n",
    "    config_text = \"\"\"\n",
    "  track_steps_per_second: 10\n",
    "  prediction_steps_per_second: 2\n",
    "  track_history_samples: 10\n",
    "  track_future_samples: 80\n",
    "  speed_lower_bound: 1.4\n",
    "  speed_upper_bound: 11.0\n",
    "  speed_scale_lower: 0.5\n",
    "  speed_scale_upper: 1.0\n",
    "  step_configurations {\n",
    "    measurement_step: 5\n",
    "    lateral_miss_threshold: 1.0\n",
    "    longitudinal_miss_threshold: 2.0\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 9\n",
    "    lateral_miss_threshold: 1.8\n",
    "    longitudinal_miss_threshold: 3.6\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 15\n",
    "    lateral_miss_threshold: 3.0\n",
    "    longitudinal_miss_threshold: 6.0\n",
    "  }\n",
    "  max_predictions: 6\n",
    "  \"\"\"\n",
    "    text_format.Parse(config_text, config)\n",
    "    return config\n",
    "\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    \"\"\"A simple one-layer regressor.\"\"\"\n",
    "\n",
    "    def __init__(self, num_agents_per_scenario, num_states_steps,\n",
    "                 num_future_steps):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self._num_agents_per_scenario = num_agents_per_scenario\n",
    "        self._num_states_steps = num_states_steps\n",
    "        self._num_future_steps = num_future_steps\n",
    "        self.regressor = tf.keras.layers.Dense(num_future_steps * 2)\n",
    "\n",
    "    def call(self, states):\n",
    "        states = tf.reshape(states, (-1, self._num_states_steps * 2))\n",
    "        pred = self.regressor(states)\n",
    "        pred = tf.reshape(\n",
    "            pred, [-1, self._num_agents_per_scenario, self._num_future_steps, 2])\n",
    "        return pred\n",
    "\n",
    "\n",
    "class MotionMetrics(tf.keras.metrics.Metric):\n",
    "    \"\"\"Wrapper for motion metrics computation.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self._prediction_trajectory = []\n",
    "        self._prediction_score = []\n",
    "        self._ground_truth_trajectory = []\n",
    "        self._ground_truth_is_valid = []\n",
    "        self._prediction_ground_truth_indices = []\n",
    "        self._prediction_ground_truth_indices_mask = []\n",
    "        self._object_type = []\n",
    "        self._metrics_config = config\n",
    "\n",
    "    def reset_state():\n",
    "        self._prediction_trajectory = []\n",
    "        self._prediction_score = []\n",
    "        self._ground_truth_trajectory = []\n",
    "        self._ground_truth_is_valid = []\n",
    "        self._prediction_ground_truth_indices = []\n",
    "        self._prediction_ground_truth_indices_mask = []\n",
    "        self._object_type = []\n",
    "\n",
    "    def update_state(self, prediction_trajectory, prediction_score,\n",
    "                     ground_truth_trajectory, ground_truth_is_valid,\n",
    "                     prediction_ground_truth_indices,\n",
    "                     prediction_ground_truth_indices_mask, object_type):\n",
    "        self._prediction_trajectory.append(prediction_trajectory)\n",
    "        self._prediction_score.append(prediction_score)\n",
    "        self._ground_truth_trajectory.append(ground_truth_trajectory)\n",
    "        self._ground_truth_is_valid.append(ground_truth_is_valid)\n",
    "        self._prediction_ground_truth_indices.append(\n",
    "            prediction_ground_truth_indices)\n",
    "        self._prediction_ground_truth_indices_mask.append(\n",
    "            prediction_ground_truth_indices_mask)\n",
    "        self._object_type.append(object_type)\n",
    "\n",
    "    def result(self):\n",
    "        # [batch_size, num_preds, 1, 1, steps, 2].\n",
    "        # The ones indicate top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "        prediction_trajectory = tf.concat(self._prediction_trajectory, 0)\n",
    "        # [batch_size, num_preds, 1].\n",
    "        prediction_score = tf.concat(self._prediction_score, 0)\n",
    "        # [batch_size, num_agents, gt_steps, 7].\n",
    "        ground_truth_trajectory = tf.concat(self._ground_truth_trajectory, 0)\n",
    "        # [batch_size, num_agents, gt_steps].\n",
    "        ground_truth_is_valid = tf.concat(self._ground_truth_is_valid, 0)\n",
    "        # [batch_size, num_preds, 1].\n",
    "        prediction_ground_truth_indices = tf.concat(\n",
    "            self._prediction_ground_truth_indices, 0)\n",
    "        # [batch_size, num_preds, 1].\n",
    "        prediction_ground_truth_indices_mask = tf.concat(\n",
    "            self._prediction_ground_truth_indices_mask, 0)\n",
    "        # [batch_size, num_agents].\n",
    "        object_type = tf.cast(tf.concat(self._object_type, 0), tf.int64)\n",
    "\n",
    "        # We are predicting more steps than needed by the eval code. Subsample.\n",
    "        interval = (\n",
    "                self._metrics_config.track_steps_per_second //\n",
    "                self._metrics_config.prediction_steps_per_second)\n",
    "        prediction_trajectory = prediction_trajectory[...,\n",
    "                                (interval - 1)::interval, :]\n",
    "\n",
    "        return py_metrics_ops.motion_metrics(\n",
    "            config=self._metrics_config.SerializeToString(),\n",
    "            prediction_trajectory=prediction_trajectory,\n",
    "            prediction_score=prediction_score,\n",
    "            ground_truth_trajectory=ground_truth_trajectory,\n",
    "            ground_truth_is_valid=ground_truth_is_valid,\n",
    "            prediction_ground_truth_indices=prediction_ground_truth_indices,\n",
    "            prediction_ground_truth_indices_mask=prediction_ground_truth_indices_mask,\n",
    "            object_type=object_type)\n",
    "\n",
    "\n",
    "model = SimpleModel(128, 11, 80)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "metrics_config = _default_metrics_config()\n",
    "motion_metrics = MotionMetrics(metrics_config)\n",
    "metric_names = config_util.get_breakdown_names_from_motion_config(\n",
    "    metrics_config)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [batch_size, num_agents, D]\n",
    "        states = inputs['input_states']\n",
    "\n",
    "        # Predict. [batch_size, num_agents, steps, 2].\n",
    "        pred_trajectory = model(states, training=True)\n",
    "\n",
    "        # Set training target.\n",
    "        prediction_start = metrics_config.track_history_samples + 1\n",
    "\n",
    "        # [batch_size, num_agents, steps, 7]\n",
    "        gt_trajectory = inputs['gt_future_states']\n",
    "        gt_targets = gt_trajectory[..., prediction_start:, :2]\n",
    "\n",
    "        # [batch_size, num_agents, steps]\n",
    "        gt_is_valid = inputs['gt_future_is_valid']\n",
    "        # [batch_size, num_agents, steps]\n",
    "        weights = (\n",
    "                tf.cast(inputs['gt_future_is_valid'][..., prediction_start:],\n",
    "                        tf.float32) *\n",
    "                tf.cast(inputs['tracks_to_predict'][..., tf.newaxis], tf.float32))\n",
    "\n",
    "        loss_value = loss_fn(gt_targets, pred_trajectory, sample_weight=weights)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # [batch_size, num_agents, steps, 2] ->\n",
    "    # [batch_size, num_agents, 1, 1, steps, 2].\n",
    "    # The added dimensions are top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "    pred_trajectory = pred_trajectory[:, :, tf.newaxis, tf.newaxis]\n",
    "\n",
    "    # Fake the score since this model does not generate any score per predicted\n",
    "    # trajectory.\n",
    "    pred_score = tf.ones(shape=tf.shape(pred_trajectory)[:3])\n",
    "\n",
    "    # [batch_size, num_agents].\n",
    "    object_type = inputs['object_type']\n",
    "\n",
    "    # [batch_size, num_agents].\n",
    "    batch_size = tf.shape(inputs['tracks_to_predict'])[0]\n",
    "    num_samples = tf.shape(inputs['tracks_to_predict'])[1]\n",
    "\n",
    "    pred_gt_indices = tf.range(num_samples, dtype=tf.int64)\n",
    "    # [batch_size, num_agents, 1].\n",
    "    pred_gt_indices = tf.tile(pred_gt_indices[tf.newaxis, :, tf.newaxis],\n",
    "                              (batch_size, 1, 1))\n",
    "    # [batch_size, num_agents, 1].\n",
    "    pred_gt_indices_mask = inputs['tracks_to_predict'][..., tf.newaxis]\n",
    "\n",
    "    motion_metrics.update_state(pred_trajectory, pred_score, gt_trajectory,\n",
    "                                gt_is_valid, pred_gt_indices,\n",
    "                                pred_gt_indices_mask, object_type)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(FILENAME)\n",
    "dataset = dataset.map(_parse)\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "epochs = 2\n",
    "num_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nStart of epoch %d' % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "\n",
    "        # Log every 10 batches.\n",
    "        if step % 10 == 0:\n",
    "            print('Training loss (for one batch) at step %d: %.4f' %\n",
    "                  (step, float(loss_value)))\n",
    "            print('Seen so far: %d samples' % ((step + 1) * 64))\n",
    "\n",
    "        if step >= num_batches_per_epoch:\n",
    "            break\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_metric_values = motion_metrics.result()\n",
    "    for i, m in enumerate(\n",
    "            ['min_ade', 'min_fde', 'miss_rate', 'overlap_rate', 'map']):\n",
    "        for j, n in enumerate(metric_names):\n",
    "            print('{}/{}: {}'.format(m, n, train_metric_values[i, j]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Waymo Open Dataset Motion Tutorial",
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
